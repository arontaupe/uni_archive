{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SickofthisShit.ipynb","provenance":[],"authorship_tag":"ABX9TyMfPDKEFReRNTUwnNAOoop+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jVXsf44anCgU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0e298a3b-66da-427b-bd29-fc9838e374bd","executionInfo":{"status":"ok","timestamp":1591633121586,"user_tz":-120,"elapsed":1922,"user":{"displayName":"Aron Petau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjySAZEocHds4D7TUo4f7t-b11sMmIcEZQ0INsQaqg=s64","userId":"01183481903912340160"}}},"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import imageio\n","import os\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","import matplotlib.pyplot as plt\n","ds, info = tfds.load('lfw', split='train', shuffle_files=True, with_info=True, as_supervised = True, download = True)\n","assert isinstance(ds, tf.data.Dataset)\n","print(ds)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["<DatasetV1Adapter shapes: ((), (250, 250, 3)), types: (tf.string, tf.uint8)>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ul_5pcLBpcHF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"1c18ef05-3ad1-42f8-87b5-8009f28aa279","executionInfo":{"status":"ok","timestamp":1591633128342,"user_tz":-120,"elapsed":1400,"user":{"displayName":"Aron Petau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjySAZEocHds4D7TUo4f7t-b11sMmIcEZQ0INsQaqg=s64","userId":"01183481903912340160"}}},"source":["info.features"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FeaturesDict({\n","    'image': Image(shape=(250, 250, 3), dtype=tf.uint8),\n","    'label': Text(shape=(), dtype=tf.string),\n","})"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"SpIFikW2tzeS","colab_type":"code","colab":{}},"source":["def visualize(embedding, label, epoch=0, acc=0., picname=''):\n","\n","    batch_size, embedding_dim = embedding.shape\n","    if embedding_dim == 2:\n","        \"\"\"\n","        visualize embedding in 2D\n","        \"\"\"\n","        fig,ax = plt.subplots()\n","        X, Y = embedding[:,0], embedding[:,1]\n","        ax.set_xlim(X.min(), X.max())\n","        ax.set_ylim(Y.min(), Y.max())\n","        for x,y,l in zip(X,Y,label):\n","            c = cm.rainbow(int(255 *l/ 9))\n","            ax.text(x, y, l, color=c)\n","            # plt.plot(x,y, '.', c=c)\n","            plt.title(\"epoch: %2d   accuracy: %.4f\" %(epoch+1, acc))\n","        plt.axis('off')\n","        plt.legend()\n","        plt.tight_layout()\n","        plt.savefig(picname)\n","\n","    if embedding_dim == 3:\n","        \"\"\"\n","        visualize embedding in 3D\n","        \"\"\"\n","        fig = plt.figure(); ax = Axes3D(fig)\n","        X, Y, Z = embedding[:, 0], embedding[:, 1], embedding[:, 2]\n","        for x, y, z, s in zip(X, Y, Z, label):\n","            c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, color=c)\n","            # c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)\n","        ax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())\n","        plt.title(\"accuracy: %.4f\" %acc)\n","        plt.legend()\n","        plt.tight_layout()\n","        plt.savefig(picname)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BrirIfd4vEhE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8433f2fb-ae3c-4beb-8241-fbf8f2cdbaff","executionInfo":{"status":"ok","timestamp":1591633273273,"user_tz":-120,"elapsed":3884,"user":{"displayName":"Aron Petau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjySAZEocHds4D7TUo4f7t-b11sMmIcEZQ0INsQaqg=s64","userId":"01183481903912340160"}}},"source":["!mkdir /content/images"],"execution_count":32,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/images’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nwcK4DcHt3Yi","colab_type":"code","colab":{}},"source":["def create_gif(gif_name, path, duration = 0.2):\n","    frames = []\n","    pngFiles = os.listdir(path)\n","    image_list = [os.path.join(path, str(i)+'.jpg') for i in range(len(pngFiles))]\n","    for image_name in image_list:\n","        frames.append(imageio.imread(image_name))\n","    imageio.mimsave(gif_name, frames, 'GIF', duration = duration)\n","    return\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgI1ngsIofIY","colab_type":"code","colab":{}},"source":["class Model(object):\n","\n","    def __init__(self, ds, embedding_dim,loss_type = 0):\n","        self.images = \n","        self.labels = labels\n","        self.embedding_dim = embedding_dim\n","        self.loss_type = loss_type\n","        self.embeddings = self.__get_embeddings()\n","        self.pred_prob, self.loss = self.__get_loss()\n","        self.predictions = self.__get_pred()\n","        self.accuracy = self.__get_accuracy()\n","\n","\n","    def __get_embeddings(self):\n","        return self.network(inputs=self.images, embedding_dim=self.embedding_dim)\n","\n","    def __get_loss(self):\n","        if self.loss_type == 0: return self.Original_Softmax_Loss(self.embeddings, self.labels)\n","        if self.loss_type == 1: return self.Modified_Softmax_Loss(self.embeddings, self.labels)\n","        if self.loss_type == 2: return self.Angular_Softmax_Loss( self.embeddings, self.labels)\n","\n","    def __get_pred(self):\n","        return tf.argmax(self.pred_prob, axis=1)\n","\n","    def __get_accuracy(self):\n","        correct_predictions = tf.equal(self.predictions, self.labels)\n","        accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'))\n","        return accuracy\n","\n","    @staticmethod\n","    def network(inputs, embedding_dim=2):\n","\n","        def prelu(inputs, name=''):\n","            alpha = tf.get_variable(name, shape=inputs.get_shape(),\n","                                    initializer=tf.constant_initializer(0.0), dtype=inputs.dtype)\n","            return tf.maximum(alpha*inputs, inputs)\n","\n","        def conv(inputs, filters, kernel_size, strides, w_init, padding='same', suffix='', scope=None):\n","            conv_name = 'conv'+suffix\n","            relu_name = 'relu'+suffix\n","\n","            with tf.name_scope(name=scope):\n","                if w_init == 'xavier':   w_init = tf.contrib.layers.xavier_initializer(uniform=True)\n","                if w_init == 'gaussian': w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n","                input_shape = inputs.get_shape().as_list()\n","                net = tf.layers.conv2d(inputs, filters, kernel_size, strides, padding=padding,\n","                                    kernel_initializer=w_init, name=conv_name)\n","                output_shape=net.get_shape().as_list()\n","                print(\"=================================================================================\")\n","                print(\"layer:%8s    input shape:%8s   output shape:%8s\" %(conv_name, str(input_shape), str(output_shape)))\n","                print(\"---------------------------------------------------------------------------------\")\n","                net = prelu(net, name=relu_name)\n","                return net\n","\n","        def resnet_block(net, blocks, suffix=''):\n","            n = len(blocks)\n","            for i in range(n):\n","                if n == 2 and i == 0: identity = net\n","                net = conv(inputs=net,\n","                           filters=blocks[i]['filters'],\n","                           kernel_size=blocks[i]['kernel_size'],\n","                           strides=blocks[i]['strides'],\n","                           w_init=blocks[i]['w_init'],\n","                           padding=blocks[i]['padding'],\n","                           suffix=suffix+'_'+blocks[i]['suffix'],\n","                           scope='conv'+suffix+'_'+blocks[i]['suffix'])\n","\n","                if n == 3 and i == 0: identity = net\n","            return identity + net\n","\n","        res1_3=[\n","            {'filters':64, 'kernel_size':3, 'strides':2, 'w_init':'xavier',   'padding':'same', 'suffix':'1'},\n","            {'filters':64, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'2'},\n","            {'filters':64, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'3'},\n","        ]\n","\n","        res2_3=[\n","            {'filters':128, 'kernel_size':3, 'strides':2, 'w_init':'xavier',   'padding':'same', 'suffix':'1'},\n","            {'filters':128, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'2'},\n","            {'filters':128, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'3'},\n","        ]\n","\n","        res2_5=[\n","            {'filters':128, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'4'},\n","            {'filters':128, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'5'},\n","        ]\n","\n","        res3_3=[\n","            {'filters':256, 'kernel_size':3, 'strides':2, 'w_init':'xavier',   'padding':'same', 'suffix':'1'},\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'2'},\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'3'},\n","        ]\n","\n","        res3_5=[\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'4'},\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'5'},\n","        ]\n","\n","        res3_7=[\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'6'},\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'7'},\n","        ]\n","\n","        res3_9=[\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'8'},\n","            {'filters':256, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'9'},\n","        ]\n","\n","        res4_3=[\n","            {'filters':512, 'kernel_size':3, 'strides':2, 'w_init':'xavier',   'padding':'same', 'suffix':'1'},\n","            {'filters':512, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'2'},\n","            {'filters':512, 'kernel_size':3, 'strides':1, 'w_init':'gaussian', 'padding':'same', 'suffix':'3'},\n","        ]\n","\n","        net = inputs\n","        for suffix, blocks in zip(('1','2','2','3','3','3','3','4'),\n","                                  (res1_3,res2_3,res2_5,res3_3,res3_5,res3_7,res3_9,res4_3)):\n","            net = resnet_block(net, blocks, suffix=suffix)\n","\n","        net = tf.layers.flatten(net)\n","        embeddings = tf.layers.dense(net, units=embedding_dim, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n","        return embeddings\n","\n","\n","    # @staticmethod\n","    # def network(inputs, embedding_dim=2, weight_decay=0.0):\n","        # \"\"\"\n","        # This is a simple convolutional neural network to extract features from images\n","        # @inputs: images (batch_size, 28, 28, 1); embedding_dim , the num of dimension of embeddings\n","        # @return: embeddings (batch_size, embedding_dim)\n","        # \"\"\"\n","        # w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n","        # with tf.name_scope('conv1.x'):\n","            # net = tf.layers.conv2d(inputs, 32, [5,5], strides=1, padding='same', kernel_initializer=w_init)\n","            # net = tf.layers.conv2d(net,    32, [5,5], strides=2, padding='same', kernel_initializer=w_init)\n","            # net = tf.nn.relu(net)\n","        # with tf.name_scope('conv2.x'):\n","            # net = tf.layers.conv2d(net,    64, [5,5], strides=1, padding='same', kernel_initializer=w_init)\n","            # net = tf.layers.conv2d(net,    64, [5,5], strides=2, padding='same', kernel_initializer=w_init)\n","            # net = tf.nn.relu(net)\n","        # with tf.name_scope('conv3.x'):\n","            # net = tf.layers.conv2d(net,   128, [5,5], strides=1, padding='same',kernel_initializer=w_init)\n","            # net = tf.layers.conv2d(net,   128, [5,5], strides=2, padding='same',kernel_initializer=w_init)\n","            # net = tf.nn.relu(net)\n","        # net = tf.layers.flatten(net)\n","        # embeddings = tf.layers.dense(net, units=embedding_dim, kernel_initializer=w_init)\n","        # return embeddings\n","\n","    @staticmethod\n","    def Original_Softmax_Loss(embeddings, labels):\n","        \"\"\"\n","        This is the orginal softmax loss, nothing to say\n","        \"\"\"\n","        with tf.variable_scope(\"softmax\"):\n","            weights = tf.get_variable(name='embedding_weights',\n","                                      shape=[embeddings.get_shape().as_list()[-1], 10],\n","                                      initializer=tf.contrib.layers.xavier_initializer())\n","            logits = tf.matmul(embeddings, weights)\n","            pred_prob = tf.nn.softmax(logits=logits) # output probability\n","            # define cross entropy\n","            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","            return pred_prob, loss\n","\n","    @staticmethod\n","    def Modified_Softmax_Loss(embeddings, labels):\n","        \"\"\"\n","        This kind of loss is slightly different from the orginal softmax loss. the main difference\n","        lies in that the L2-norm of the weights are constrained  to 1, then the\n","        decision boundary will only depends on the angle between weights and embeddings.\n","        \"\"\"\n","        # # normalize embeddings\n","        # embeddings_norm = tf.norm(embeddings, axis=1, keepdims=True)\n","        # embeddings = tf.div(embeddings, embeddings_norm, name=\"normalize_embedding\")\n","        \"\"\"\n","        the abovel commented-out code would lead loss to divergence, maybe you can try it.\n","        \"\"\"\n","        with tf.variable_scope(\"softmax\"):\n","            weights = tf.get_variable(name='embedding_weights',\n","                                      shape=[embeddings.get_shape().as_list()[-1], 10],\n","                                      initializer=tf.contrib.layers.xavier_initializer())\n","            # normalize weights\n","            weights_norm = tf.norm(weights, axis=0, keepdims=True)\n","            weights = tf.div(weights, weights_norm, name=\"normalize_weights\")\n","            logits = tf.matmul(embeddings, weights)\n","            pred_prob = tf.nn.softmax(logits=logits)\n","            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","            return pred_prob, loss\n","\n","    @staticmethod\n","    def Angular_Softmax_Loss(embeddings, labels, margin=4):\n","        \"\"\"\n","        Note:(about the value of margin)\n","        as for binary-class case, the minimal value of margin is 2+sqrt(3)\n","        as for multi-class  case, the minimal value of margin is 3\n","        the value of margin proposed by the author of paper is 4.\n","        here the margin value is 4.\n","        \"\"\"\n","        l = 0.\n","        embeddings_norm = tf.norm(embeddings, axis=1)\n","\n","        with tf.variable_scope(\"softmax\"):\n","            weights = tf.get_variable(name='embedding_weights',\n","                                      shape=[embeddings.get_shape().as_list()[-1], 10],\n","                                      initializer=tf.contrib.layers.xavier_initializer())\n","            weights = tf.nn.l2_normalize(weights, axis=0)\n","            # cacualting the cos value of angles between embeddings and weights\n","            orgina_logits = tf.matmul(embeddings, weights)\n","            N = embeddings.get_shape()[0] # get batch_size\n","            single_sample_label_index = tf.stack([tf.constant(list(range(N)), tf.int64), labels], axis=1)\n","            # N = 128, labels = [1,0,...,9]\n","            # single_sample_label_index:\n","            # [ [0,1],\n","            #   [1,0],\n","            #   ....\n","            #   [128,9]]\n","            selected_logits = tf.gather_nd(orgina_logits, single_sample_label_index)\n","            cos_theta = tf.div(selected_logits, embeddings_norm)\n","            cos_theta_power = tf.square(cos_theta)\n","            cos_theta_biq = tf.pow(cos_theta, 4)\n","            sign0 = tf.sign(cos_theta)\n","            sign3 = tf.multiply(tf.sign(2*cos_theta_power-1), sign0)\n","            sign4 = 2*sign0 + sign3 -3\n","            result=sign3*(8*cos_theta_biq-8*cos_theta_power+1) + sign4\n","\n","            margin_logits = tf.multiply(result, embeddings_norm)\n","            f = 1.0/(1.0+l)\n","            ff = 1.0 - f\n","            combined_logits = tf.add(orgina_logits, tf.scatter_nd(single_sample_label_index,\n","                                                           tf.subtract(margin_logits, selected_logits),\n","                                                           orgina_logits.get_shape()))\n","            updated_logits = ff*orgina_logits + f*combined_logits\n","            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=updated_logits))\n","            pred_prob = tf.nn.softmax(logits=updated_logits)\n","            return pred_prob, loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuEfCgRutKQb","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","\n","\n","\n","# define training parameters\n","lr = 0.001\n","epochs = 40\n","batch_size = 256\n","train_batchs = 40 # the number of batchs per epoch\n","test_batchs  = 20\n","embedding_dim = 2 # 3\n","loss_type = 1\n","\n","def train():\n","\n","    global_step = tf.Variable(0, trainable=False)\n","    \n","    # about network\n","    model = Model(ds, embedding_dim, loss_type)\n","    accuracy = network.accuracy\n","    loss = network.loss\n","    # define optimizer and learning rate\n","    decay_lr = tf.train.exponential_decay(lr, global_step, 500, 0.9)\n","    optimizer = tf.train.AdamOptimizer(decay_lr)\n","    train_op = optimizer.minimize(loss)\n","\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","\n","    summary = tf.summary.FileWriter(\"./image/\", sess.graph)\n","\n","    for epoch in range(epochs):\n","        nlabels = np.zeros((train_batchs*batch_size,), dtype=np.int32)\n","        embeddings = np.zeros((train_batchs*batch_size, embedding_dim), dtype=np.float32)\n","        train_acc = 0.\n","        for batch in tqdm(range(train_batchs)):\n","            i,j = batch*batch_size, (batch+1)*batch_size\n","            batch_images, batch_labels = mnist.train.next_batch(batch_size)\n","            feed_dict = {images:batch_images, labels:batch_labels}\n","            _, _, batch_loss, batch_acc, embeddings[i:j,:] = sess.run([train_op, add_step_op, loss, accuracy, network.embeddings], feed_dict)\n","            nlabels[i:j] = batch_labels\n","            f.write(\" \".join(map(str,[batch_acc, batch_loss]))+ \"\\n\")\n","            print(batch_acc)\n","            train_acc += batch_acc\n","        train_acc /= train_batchs\n","        print(\"epoch %2d---------------------------train accuracy:%.4f\" %(epoch+1, train_acc))\n","        visualize(embeddings, nlabels, epoch, train_acc, picname=\"./image/%d/%d.jpg\"%(loss_type, epoch))\n","    \n","    # testing process\n","    test_acc = 0.\n","    embeddings = np.zeros((test_batchs*batch_size, embedding_dim), dtype=np.float32)\n","    nlabels = np.zeros(shape=(test_batchs*batch_size,), dtype=np.int32)\n","    for batch in range(test_batchs):\n","        i,j = batch*batch_size, (batch+1)*batch_size\n","        batch_images, batch_labels = mnist.test.next_batch(batch_size)\n","        feed_dict = {images:batch_images, labels:batch_labels}\n","        _, batch_loss, batch_acc, embeddings[i:j,:] = sess.run([train_op, loss, accuracy, network.embeddings], feed_dict)\n","        nlabels[i:j] = batch_labels\n","        test_acc += batch_acc\n","    test_acc /= test_batchs\n","    print(\"test accuracy: %.4f\" %test_acc)\n","    return test_acc, embeddings, nlabels\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3w1mDDsCx80X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":307},"outputId":"85123d5f-bacc-4d5e-e083-62229c490536","executionInfo":{"status":"error","timestamp":1591634213778,"user_tz":-120,"elapsed":3032,"user":{"displayName":"Aron Petau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjySAZEocHds4D7TUo4f7t-b11sMmIcEZQ0INsQaqg=s64","userId":"01183481903912340160"}}},"source":["print(train())"],"execution_count":46,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-56d46c7289a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-6fa8d5cb6b46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# about network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0611ff9f2f92>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images, labels, embedding_dim, loss_type)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0611ff9f2f92>\u001b[0m in \u001b[0;36m__get_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0611ff9f2f92>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(inputs, embedding_dim)\u001b[0m\n\u001b[1;32m    128\u001b[0m         for suffix, blocks in zip(('1','2','2','3','3','3','3','4'),\n\u001b[1;32m    129\u001b[0m                                   (res1_3,res2_3,res2_5,res3_3,res3_5,res3_7,res3_9,res4_3)):\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0611ff9f2f92>\u001b[0m in \u001b[0;36mresnet_block\u001b[0;34m(net, blocks, suffix)\u001b[0m\n\u001b[1;32m     76\u001b[0m                            \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'padding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                            \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'suffix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                            scope='conv'+suffix+'_'+blocks[i]['suffix'])\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0611ff9f2f92>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(inputs, filters, kernel_size, strides, w_init, padding, suffix, scope)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'xavier'\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"]}]}]}