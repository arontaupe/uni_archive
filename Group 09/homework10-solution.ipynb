{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"homework10-solution.ipynb","provenance":[{"file_id":"https://github.com/lukeeffenberger/IANNWTF-2019/blob/master/homework-solutions/homework10-solution.ipynb","timestamp":1580449104537}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JXT1XnPMG3ES","colab_type":"text"},"source":["# Homework 10 - Word Embeddings\n","\n","In this homework you will train the skip gram model on the bible."]},{"cell_type":"code","metadata":{"id":"Es3G8kEEG3EY","colab_type":"code","colab":{}},"source":["import numpy as np\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","from nltk.tokenize import RegexpTokenizer\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g8boOnsG3Ee","colab_type":"code","colab":{}},"source":["# These functions are provided to generate two dictionaries to translate words into ids and back.\n","\n","def tokenize_text(text):\n","    text_lower = text.lower()\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    text_tokenized = tokenizer.tokenize(text_lower)\n","    return text_tokenized\n","\n","def create_dicts_from_tokenized_text(tokenized_text,vocabulary_size):\n","    words_and_count = Counter(tokenized_text).most_common(vocabulary_size - 1)\n","    print(words_and_count)\n","    word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n","    word2id[\"_UNKNOWN_\"] = 0\n","    id2word = dict(zip(word2id.values(), word2id.keys()))\n","    return word2id, id2word"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vw8yl8_xG3Em","colab_type":"code","colab":{}},"source":["# Read in and tokenize the bible text.\n","bible_text = open('bible.txt').read()\n","bible_text_tokenized = tokenize_text(bible_text)\n","\n","# Create dictionaries for the 10000 most common words.\n","vocabulary_size = 10000\n","word2id, id2word = create_dicts_from_tokenized_text(bible_text_tokenized, vocabulary_size)\n","\n","# Translate the tokenized text into ids.\n","bible_id = [word2id.get(word, 0) for word in bible_text_tokenized]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"coCS1R71G3Et","colab_type":"code","colab":{}},"source":["# Generate the dataset of all pairs of word and context with a context window of 2.\n","# Tip: Create two lists: \n","#          - one long list with all valid word indices (for which you can apply the context windows)\n","#          - one short list with the shifts for getting the context word (remember to exclude 0)\n","#      Then generate two lists (input words + context words), by running through the two lists above.\n","\n","### YOUR CODE HERE ###\n","# Define the context range.\n","context_range = 2\n","context_window = [i for i in range(-context_range, context_range + 1) if i != 0]\n","\n","# Only use the words that have enough words before and after it for getting the contexts.\n","valid_indices = range(context_range, len(bible_id) - (context_range))\n","\n","# Generate one list of the words and one for the contexts. If you have e.g. 4 context words for each word\n","# remember that you need each word 4 times in the list for the words.\n","word_ids = [bible_id[word_index] for word_index in valid_indices for shift in context_window]\n","context_ids = [bible_id[word_index + shift] for word_index in valid_indices for shift in context_window]\n","######################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qz8QN5cuG3E_","colab_type":"code","colab":{}},"source":["# Create a dataset from these lists. Batch size: 128. Shuffle.\n","### YOUR CODE HERE ###\n","dataset = tf.data.Dataset.from_tensor_slices((word_ids,context_ids))\n","dataset = dataset.shuffle(buffer_size=10000)\n","dataset = dataset.batch(128)\n","######################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpxVckCzG3FV","colab_type":"code","colab":{}},"source":["class SkipGram(tf.keras.layers.Layer):\n","    \n","    def __init__(self, vocab_size, embedding_size):\n","        super(SkipGram, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_size = embedding_size\n","        \n","        \n","    def build(self, input_shape):\n","        self.embedding_matrix = self.add_weight(\n","                                shape=(self.vocab_size, self.embedding_size),\n","                                initializer='GlorotNormal'\n","                                )\n","        self.score_matrix = self.add_weight(\n","                            shape=(self.vocab_size, self.embedding_size),\n","                            initializer='GlorotNormal'\n","                            )\n","        self.score_bias = self.add_weight(\n","                            shape=(self.vocab_size),\n","                            initializer='zeros'\n","                            )\n","        \n","    def call(self,inputs,labels):\n","        ### YOUR CODE HERE ###\n","        labels = tf.expand_dims(labels, axis=-1)\n","        # Get the embeddings. Use tf.nn.embedding_lookup().\n","        embeddings = tf.nn.embedding_lookup(self.embedding_matrix,inputs)\n","        # Instead of calculating the scores, we will directly calculate and return the loss.\n","        # Use tf.nn.nce_loss(). Remember to average the loss over all batches.\n","        nce_loss = tf.nn.nce_loss(\n","                    weights = self.score_matrix,\n","                    biases = self.score_bias,\n","                    labels = labels,\n","                    inputs = embeddings,\n","                    num_sampled = 15,\n","                    num_classes = self.vocab_size\n","        )\n","        loss = tf.reduce_mean(nce_loss)\n","        return loss\n","        #######################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-7BGaF9G3Fb","colab_type":"code","colab":{}},"source":["# Provided function to readout the nearest neighbors inside the embedding.\n","# Feel free to add more words to the list.\n","target_words = ['israel', 'god', 'jesus', '1', 'love', 'day', 'wine']\n","number_of_nearest_neighbors = 8\n","\n","def find_and_print_nearest_neighbors(target_words, number_of_nearest_neighbors,embeddings):\n","    normed_embeddings = embeddings / np.sqrt(np.sum(embeddings**2, axis=1, keepdims=True))\n","    for word in target_words:\n","        word_id = word2id[word]\n","        word_embedding = normed_embeddings[word_id, :]\n","        cosine_similarities = np.matmul(normed_embeddings, word_embedding )\n","        n_nearest_neighbors = np.argsort(-cosine_similarities)[:number_of_nearest_neighbors]\n","        print(\"Nearest to \" + word + \": \" + \", \".join([id2word[nearest] for nearest in n_nearest_neighbors]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUkYp3UJG3Fg","colab_type":"code","colab":{}},"source":["tf.keras.backend.clear_session()\n","\n","### YOUR CODE HERE ###\n","# Initialize model (vocab size:10000, embedding size:64) and optimizer (Adam, lr:1).\n","model = SkipGram(vocab_size=vocabulary_size, embedding_size=64)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1)\n","\n","# Train model for at least 10 epochs. This might take a while. \n","# But you should see first sensible results already after 2 or 3 epochs.\n","for epoch in range(10):\n","    for x,t in dataset:\n","\n","        # Compute loss, gradients and apply gradients.\n","        with tf.GradientTape() as tape:\n","            loss = model(x,t)\n","            gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        \n","    # Once after the epoch use the above defined function to print the nearest neighbors.\n","    # Get the current embeddings via the model.trainable_variables parameter.\n","    embeddings = model.trainable_variables[0].numpy()\n","    print('Epoch: {}'.format(epoch))\n","    find_and_print_nearest_neighbors(target_words, 8, embeddings)\n","    print('------------------------')\n","#####################"],"execution_count":0,"outputs":[]}]}